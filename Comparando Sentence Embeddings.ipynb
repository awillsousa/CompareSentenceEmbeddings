{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de 4 técnicas de sentence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artigo original de onde veio essa tralha toda: https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/\n",
    "\n",
    "Comparação entre as 4 técnicas a seguir:\n",
    "\n",
    "<ol>\n",
    "    <li>Doc2Vec</li>\n",
    "    <li>SentenceBERT</li>\n",
    "    <li>InferSent</li>\n",
    "    <li>Universal Sentence Encoder</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação do ambiente e Configurações básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/willian/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155/nltk-3.5-py3-none-any.whl\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp38-cp38-manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[K     |████████████████████████████████| 738 kB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, click, tqdm, regex, nltk\n",
      "Successfully installed click-7.1.2 joblib-0.17.0 nltk-3.5 regex-2020.11.13 tqdm-4.51.0\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 10.0 MB/s eta 0:00:01   |███                             | 1.3 MB 6.1 MB/s eta 0:00:03\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.19.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/willian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I ate dinner.\", \n",
    "       \"We had a three-course meal.\", \n",
    "       \"Brad came to dinner with us.\",\n",
    "       \"He loves fish tacos.\",\n",
    "       \"In the end, we all felt like we ate too much.\",\n",
    "       \"We all agreed; it was a magnificent evening.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'ate', 'dinner', '.'],\n",
       " ['we', 'had', 'a', 'three-course', 'meal', '.'],\n",
       " ['brad', 'came', 'to', 'dinner', 'with', 'us', '.'],\n",
       " ['he', 'loves', 'fish', 'tacos', '.'],\n",
       " ['in',\n",
       "  'the',\n",
       "  'end',\n",
       "  ',',\n",
       "  'we',\n",
       "  'all',\n",
       "  'felt',\n",
       "  'like',\n",
       "  'we',\n",
       "  'ate',\n",
       "  'too',\n",
       "  'much',\n",
       "  '.'],\n",
       " ['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizacao de cada sentença\n",
    "tokenized_sent = []\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para calcular a similaridade coseno\n",
    "\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extension of Word2Vec, the Doc2Vec embedding is one of the most popular techniques out there. Introduced in 2014, it is an unsupervised algorithm and adds on to the Word2Vec model by introducing another ‘paragraph vector’. Also, there are 2 ways to add the paragraph vector to the model.\n",
    "\n",
    "1.1) PVDM(Distributed Memory version of Paragraph Vector): We assign a paragraph vector sentence while sharing word vectors among all sentences. Then we either average or concatenate the (paragraph vector and words vector) to get the final sentence representation. If you notice, it is an extension of the Continuous Bag-of-Word type of Word2Vec where we predict the next word given a set of words. It is just that in PVDM, we predict the next sentence given a set of sentences.\n",
    "\n",
    "![title](PVDM.png)\n",
    "\n",
    "1.2) PVDOBW( Distributed Bag of Words version of Paragraph Vector): Just lime PVDM, PVDOBW is another extension, this time of the Skip-gram type. Here, we just sample random words from the sentence and make the model predict which sentence it came from(a classification task).\n",
    "\n",
    "![title](PVDOBW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "Collecting scipy>=0.18.1\n",
      "  Downloading scipy-1.5.4-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.8 MB 534 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from gensim) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107095 sha256=28f21266d1515bbe1f01a34c88a2a07d6479fa9aabe053b511b675fe31cededa\n",
      "  Stored in directory: /home/willian/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: scipy, idna, chardet, urllib3, requests, smart-open, gensim\n",
      "Successfully installed chardet-3.0.4 gensim-3.8.3 idna-2.10 requests-2.25.0 scipy-1.5.4 smart-open-3.0.0 urllib3-1.26.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'ate', 'dinner', '.'], tags=[0]),\n",
       " TaggedDocument(words=['we', 'had', 'a', 'three-course', 'meal', '.'], tags=[1]),\n",
       " TaggedDocument(words=['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], tags=[2]),\n",
       " TaggedDocument(words=['he', 'loves', 'fish', 'tacos', '.'], tags=[3]),\n",
       " TaggedDocument(words=['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much', '.'], tags=[4]),\n",
       " TaggedDocument(words=['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.'], tags=[5])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': <gensim.models.keyedvectors.Vocab at 0x7f8baed06700>,\n",
       " 'ate': <gensim.models.keyedvectors.Vocab at 0x7f8baed06c10>,\n",
       " 'dinner': <gensim.models.keyedvectors.Vocab at 0x7f8baed069d0>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x7f8baed06c70>,\n",
       " 'we': <gensim.models.keyedvectors.Vocab at 0x7f8baed06ee0>,\n",
       " 'had': <gensim.models.keyedvectors.Vocab at 0x7f8baed06fd0>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x7f8baed06a00>,\n",
       " 'three-course': <gensim.models.keyedvectors.Vocab at 0x7f8baed06fa0>,\n",
       " 'meal': <gensim.models.keyedvectors.Vocab at 0x7f8baed06e20>,\n",
       " 'brad': <gensim.models.keyedvectors.Vocab at 0x7f8baed067c0>,\n",
       " 'came': <gensim.models.keyedvectors.Vocab at 0x7f8baed06820>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x7f8bc222c790>,\n",
       " 'with': <gensim.models.keyedvectors.Vocab at 0x7f8bc222ca30>,\n",
       " 'us': <gensim.models.keyedvectors.Vocab at 0x7f8bc222cac0>,\n",
       " 'he': <gensim.models.keyedvectors.Vocab at 0x7f8bc222cdc0>,\n",
       " 'loves': <gensim.models.keyedvectors.Vocab at 0x7f8bc222cb50>,\n",
       " 'fish': <gensim.models.keyedvectors.Vocab at 0x7f8bc222cfd0>,\n",
       " 'tacos': <gensim.models.keyedvectors.Vocab at 0x7f8bc222cc40>,\n",
       " 'in': <gensim.models.keyedvectors.Vocab at 0x7f8bc222ce50>,\n",
       " 'the': <gensim.models.keyedvectors.Vocab at 0x7f8bc222c8e0>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x7f8bc222c820>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x7f8bc222c670>,\n",
       " 'all': <gensim.models.keyedvectors.Vocab at 0x7f8bc222cbb0>,\n",
       " 'felt': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c310>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c070>,\n",
       " 'too': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c0d0>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c130>,\n",
       " 'agreed': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c190>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c1f0>,\n",
       " 'it': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c250>,\n",
       " 'was': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c2b0>,\n",
       " 'magnificent': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c340>,\n",
       " 'evening': <gensim.models.keyedvectors.Vocab at 0x7f8baed1c3a0>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n",
    "\n",
    "## Print model vocabulary\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take up a new test sentence and find the top 5 most similar sentences from our data.<br>\n",
    "We will also display them in order of decreasing similarity. <br>\n",
    "The infer_vector method returns the vectorized form of the test sentence(including the paragraph vector). <br>\n",
    "The most_similar method returns similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npositive = List of sentences that contribute positively.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.docvecs.most_similar(positive = [test_doc_vector])\n",
    "\n",
    "'''\n",
    "positive = List of sentences that contribute positively.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the leader among the pack, SentenceBERT was introduced in 2018 and immediately took the pole position for Sentence Embeddings. At the heart of this BERT-based model, there are 4 key concepts:\n",
    "<ul>\n",
    "    <li>Attention</li>\n",
    "    <li>Transformers</li>\n",
    "    <li>BERT</li>\n",
    "    <li>Siamese Network</li>\n",
    "</ul>\n",
    " \n",
    "\n",
    "Sentence-BERT uses a Siamese network like architecture to provide 2 sentences as an input. These 2 sentences are then passed to BERT models and a pooling layer to generate their embeddings. Then use the embeddings for the pair of sentences as inputs to calculate the cosine similarity.\n",
    "    \n",
    "![SBERT](SBERT.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 3.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from sentence-transformers) (4.51.0)\n",
      "Collecting torch>=1.2.0\n",
      "  Downloading torch-1.7.0-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8 MB 99 bytes/s ta 0:00:01  |                                | 583 kB 7.8 MB/s eta 0:01:40     |                                | 2.4 MB 7.8 MB/s eta 0:01:39     |██████▋                         | 159.5 MB 4.2 MB/s eta 0:02:29     |██████████████████▉             | 457.7 MB 6.0 MB/s eta 0:00:54     |███████████████████             | 463.4 MB 6.0 MB/s eta 0:00:53     |████████████████████████▌       | 595.3 MB 3.5 MB/s eta 0:00:53     |██████████████████████████      | 632.6 MB 1.3 MB/s eta 0:01:56     |████████████████████████████▏   | 683.4 MB 7.9 MB/s eta 0:00:12\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from sentence-transformers) (1.19.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 9.0 MB/s eta 0:00:01     |████████████████████▏           | 4.3 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from sentence-transformers) (1.5.4)\n",
      "Requirement already satisfied: nltk in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from sentence-transformers) (3.5)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.11.13)\n",
      "Processing /home/willian/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677/sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n",
      "Requirement already satisfied: requests in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.25.0)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hProcessing /home/willian/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4/future-0.18.2-py3-none-any.whl\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: click in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.6.20)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-py3-none-any.whl size=101996 sha256=96e1dbc3e3420affdb4806129d400cf56eb32badb0b4ce1960a129be5802a1d1\n",
      "  Stored in directory: /home/willian/.cache/pip/wheels/95/eb/10/39e6fdf924ec09c8f177547a8b3976c624632e180d36e1cb56\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: filelock, tokenizers, sacremoses, sentencepiece, transformers, future, dataclasses, typing-extensions, torch, threadpoolctl, scikit-learn, sentence-transformers\n",
      "Successfully installed dataclasses-0.6 filelock-3.0.12 future-0.18.2 sacremoses-0.0.43 scikit-learn-0.23.2 sentence-transformers-0.3.8 sentencepiece-0.1.94 threadpoolctl-2.1.0 tokenizers-0.8.1rc2 torch-1.7.0 transformers-3.3.1 typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then load the pre-trained BERT model. There are many other pre-trained models available.<br>\n",
    "\n",
    "A full list of models here: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405M/405M [01:01<00:00, 6.63MB/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then encode the provided sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BERT embedding vector - length 768\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = sbert_model.encode(sentences)\n",
    "\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "#print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will define a test query and encode it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I had pizza and pasta\"\n",
    "query_vec = sbert_model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then compute the cosine similarity using scipy. <br>We will retrieve the similarity values between the sentences and our test query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I ate dinner. ; similarity =  0.7173461\n",
      "Sentence =  We had a three-course meal. ; similarity =  0.637134\n",
      "Sentence =  Brad came to dinner with us. ; similarity =  0.5897908\n",
      "Sentence =  He loves fish tacos. ; similarity =  0.62239355\n",
      "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.41980496\n",
      "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.18081594\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "  sim = cosine(query_vec, sbert_model.encode([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented by Facebook AI Research in 2018, InferSent is a supervised sentence embedding technique. The main feature of this model is that it is trained on Natural language Inference(NLI) data, more specifically, the SNLI (Stanford Natural Language Inference) dataset. It consists of 570k human-generated English sentence pairs, manually labeled with one of the three categories – entailment, contradiction, or neutral.\n",
    "\n",
    "Just like SentenceBERT, we take a pair of sentences and encode them to generate the actual sentence embeddings. Then, extract the relations between these embeddings using:\n",
    "<ul>\n",
    "    <li>concatenation</li>\n",
    "    <li>element-wise product</li>\n",
    "    <li>absolute element-wise difference.</li>\n",
    "</ul>\n",
    "\n",
    "![InferSent](infersent.webp)\n",
    "\n",
    "The output vector of these operations is then fed to a classifier that classifies the vector into one of the 3 above-defined categories. The actual paper proposes various encoder architectures, majorly concentrated around GRUs, LSTMs, and BiLSTMs.\n",
    "\n",
    "Another important feature is that InferSent uses GloVe vectors for pre-trained word embeddings. A more recent version of InferSent, known as InferSent2 uses fastText.\n",
    "\n",
    "Let us see how Sentence Similarity task works using InferSent. We will use PyTorch for this, so do make sure that you have the latest PyTorch version installed from here: https://pytorch.org/get-started/locally/#mac-anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, there are 2 versions of InferSent. Version 1 uses GLovE while version 2 uses fastText vectors. You can choose to work with any model (I have used version 2). Thus, we download the InferSent Model and the pre-trained Word Vectors. For this, please first save the models.py file from here and store it in your working directory.\n",
    "\n",
    "We also need to save the trained model and pre-trained GLoVe word vectors. According to the code below, our working directory should have an ‘encoders’ folder and a folder called ‘GLoVe’. The encoder folder will have our model while the GloVe folder should have the word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.3-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from matplotlib) (1.18.5)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.0.1-cp38-cp38-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Installing collected packages: cycler, kiwisolver, pillow, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.3 pillow-8.0.1\n"
     ]
    }
   ],
   "source": [
    "#! mkdir encoder\n",
    "#! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
    "  \n",
    "#! mkdir GloVe\n",
    "#! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "#! unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load our model and our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "import torch\n",
    "\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = 'GloVe/glove.840B.300d.txt'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we build the vocabulary from the list of sentences that we defined at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36(/36) words with w2v vectors\n",
      "Vocab size : 36\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences, tokenize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we have the test query and we use InferSent to encode this test query and generate an embedding for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/willian/desenv/master/SentenceEmbeddings/models.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sentences = np.array(sentences)[idx_sort]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.02459561,  0.04943122, -0.15705207, ...,  0.07534432,\n",
       "       -0.03941802,  0.05388859], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"I had pizza and pasta\"\n",
    "query_vec = model.encode(query)[0]\n",
    "query_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the cosine similarity of this query with each sentence in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I ate dinner. ; similarity =  0.6868881\n",
      "Sentence =  We had a three-course meal. ; similarity =  0.50432706\n",
      "Sentence =  Brad came to dinner with us. ; similarity =  0.55740434\n",
      "Sentence =  He loves fish tacos. ; similarity =  0.590714\n",
      "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.57681197\n",
      "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.5049965\n"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "for sent in sentences:\n",
    "  sim = cosine(query_vec, model.encode([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0246,  0.1410, -0.1445,  ...,  0.1425, -0.0003,  0.0578]],\n",
       "        grad_fn=<MaxBackward0>),\n",
       " array([[1, 2, 2, ..., 1, 3, 1]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.visualize(sentences[0], tokenize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most well-performing sentence embedding techniques right now is the Universal Sentence Encoder. <br>\n",
    "And it should come as no surprise from anybody that it has been proposed by Google. The key feature here is that <br>\n",
    "we can use it for Multi-task learning. \n",
    "<br>\n",
    "This means that the sentence embeddings we generate can be used for multiple tasks like sentiment analysis, <br>\n",
    "text classification, sentence similarity, etc, and the results of these asks are then fed back to the model to get <br>\n",
    "even better sentence vectors that before.\n",
    "<br>\n",
    "The most interesting part is that this encoder is based on two encoder models and we can use either of the two:\n",
    "    \n",
    "<ul>    \n",
    "    <li>Transformer</li>\n",
    "    <li>Deep Averaging Network(DAN)</li>\n",
    "</ul>    \n",
    "\n",
    "Both of these models are capable of taking a word or a sentence as input and generating embeddings for the same. The following is the basic flow:\n",
    "<ol>\n",
    "    <li>Tokenize the sentences after converting them to lowercase</li>\n",
    "    <li>Depending on the type of encoder, the sentence gets converted to a 512-dimensional vector\n",
    "        <ul>\n",
    "            <li>If we use the transformer, it is similar to the encoder module of the transformer architecture and uses the self-attention mechanism.</li>\n",
    "            <li>The DAN option computes the unigram and bigram embeddings first and then averages them to get a single embedding. This is then passed to a deep neural network to get a final sentence embedding of 512 dimensions.</li>\n",
    "        </ul>\n",
    "        </li>    \n",
    "    <li>These sentence embeddings are then used for various unsupervised and supervised tasks like Skipthoughts, NLI, etc. The trained model is then again reused to generate a new 512 dimension sentence embedding.</li>\n",
    "<ol>\n",
    "    \n",
    "![USE](USE.webp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start using the USE embedding, we first need to install TensorFlow and TensorFlow hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.3.1-cp38-cp38-manylinux2010_x86_64.whl (320.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 320.5 MB 9.7 kB/s eta 0:00:011  |▉                               | 8.1 MB 5.1 MB/s eta 0:01:02     |████████▍                       | 83.9 MB 7.5 MB/s eta 0:00:32     |████████████████████▏           | 201.4 MB 6.2 MB/s eta 0:00:20     |████████████████████▎           | 203.4 MB 6.2 MB/s eta 0:00:19     |██████████████████████████████▉ | 308.7 MB 10.5 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Using cached numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.2-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.25.0)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (50.3.1.post20201107)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.26.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=021bbaa60e3ecf091f90bea32e93026c86b97347cfaab45d361d37fedf73ec6a\n",
      "  Stored in directory: /home/willian/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78079 sha256=bd7e5593e9d7504f9252251781a4edb35dc448c87113128a26c8dd19ebe807cc\n",
      "  Stored in directory: /home/willian/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: pyasn1, rsa, cachetools, pyasn1-modules, google-auth, tensorboard-plugin-wit, grpcio, protobuf, absl-py, werkzeug, oauthlib, requests-oauthlib, google-auth-oauthlib, numpy, markdown, tensorboard, tensorflow-estimator, keras-preprocessing, termcolor, astunparse, gast, wrapt, h5py, opt-einsum, google-pasta, tensorflow-gpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.4\n",
      "    Uninstalling numpy-1.19.4:\n",
      "      Successfully uninstalled numpy-1.19.4\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.1 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.10.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from tensorflow-hub) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from tensorflow-hub) (1.18.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/willian/anaconda3/envs/SentenceEmbeddings/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade tensorflow-gpu\n",
    "# Install TF-Hub.\n",
    "!pip3 install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will generate embeddings for our sentence list as well as for our query. This is as simple as just passing the sentences to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = model(sentences)\n",
    "query = \"I had pizza and pasta\"\n",
    "query_vec = model([query])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, we will compute the similarity between our test query and the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence =  I ate dinner. ; similarity =  0.46866417\n",
      "Sentence =  We had a three-course meal. ; similarity =  0.35643065\n",
      "Sentence =  Brad came to dinner with us. ; similarity =  0.20338945\n",
      "Sentence =  He loves fish tacos. ; similarity =  0.16515438\n",
      "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.14987424\n",
      "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.058435913\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "  sim = cosine(query_vec, model([sent])[0])\n",
    "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
